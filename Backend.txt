AI Data Analyst - Complete Backend PRD
1. Project Overview
Product Name: AI Data Analyst
Core Functionality: Natural language interface for CSV/XLSX data analysis with SQL generation, visualization, and intelligent query understanding
Key Features:

Upload and process CSV/XLSX files
Natural language to SQL query generation
Automatic data visualization
Context-aware conversation handling
Multi-turn dialogue with query refinement


2. System Architecture
2.1 Architecture Pattern

Type: Microservices/Modular Monolith
Communication: RESTful APIs
Data Flow: Request → NLP Processing → SQL Generation → Execution → Visualization → Response

2.2 Core Components
┌─────────────────┐
│   API Gateway   │
└────────┬────────┘
         │
    ┌────┴─────┬──────────┬───────────┐
    │          │          │           │
┌───▼───┐  ┌──▼──┐  ┌────▼────┐  ┌──▼───┐
│Upload │  │ NLP │  │Query    │  │Viz   │
│Service│  │Engine│ │Executor │  │Engine│
└───────┘  └─────┘  └─────────┘  └──────┘
                │
         ┌──────▼──────┐
         │   SQLite    │
         │  (In-Memory)│
         └─────────────┘

3. Technology Stack
3.1 Backend Framework

Primary: Node.js with Express.js (Fast, scalable)
Alternative: Python with FastAPI (Better ML integration)

Recommended: Python + FastAPI for easier AI integration
3.2 AI/LLM Integration

Provider: Anthropic Claude API (Claude Sonnet 4.5)
Reason: Superior SQL generation, context understanding, structured outputs
Library: anthropic Python SDK

3.3 Database & Query Engine

In-Memory Database: SQLite (loaded from CSV/XLSX)
Library: sqlite3 (Python) or better-sqlite3 (Node.js)
CSV/XLSX Processing:

Python: pandas, openpyxl
Node.js: xlsx, papaparse



3.4 Visualization

Approach: Return visualization configs, not images
Format: JSON specification for charts
Frontend Libraries: Chart.js, Recharts, or Plotly.js
Backend Role: Determine chart type and data structure

3.5 Additional Libraries

Validation: pydantic (Python) or zod (Node.js)
File Upload: multer (Node.js) or python-multipart (FastAPI)
CORS: fastapi.middleware.cors or cors package
Environment: python-dotenv or dotenv


4. Core Modules & Logic
4.1 File Upload & Processing Module
Responsibilities:

Accept CSV/XLSX uploads (max 50MB)
Parse and validate file structure
Load into SQLite in-memory database
Generate schema metadata
Return table structure to frontend

Logic Flow:
1. Receive file upload
2. Validate file type and size
3. Parse with pandas (CSV/XLSX)
4. Infer data types automatically
5. Create SQLite table with appropriate types
6. Insert data into SQLite
7. Generate schema JSON:
   {
     "table_name": "data",
     "columns": [
       {"name": "col1", "type": "INTEGER", "sample_values": [...]},
       {"name": "col2", "type": "TEXT", "sample_values": [...]}
     ],
     "row_count": 1000
   }
8. Store session context
Key Functions:

parse_file(file) → DataFrame
infer_schema(df) → Schema object
load_to_sqlite(df, session_id) → Connection
get_table_metadata(conn) → JSON schema


4.2 NLP Query Understanding Module
Responsibilities:

Understand user intent
Identify required columns and operations
Handle ambiguous queries
Maintain conversation context

Logic Flow:
1. Receive user question
2. Retrieve conversation history (last 5 turns)
3. Retrieve table schema
4. Send to Claude with specialized prompt
5. Extract:
   - Intent (aggregate, filter, trend, compare, etc.)
   - Required columns
   - Operations needed
   - Clarification questions (if ambiguous)
6. Return structured response

4.3 SQL Generation Module
Responsibilities:

Convert natural language to SQL
Ensure SQL safety and correctness
Handle complex queries (JOINs, aggregations, window functions)
Validate against schema

The Critical Prompt (CamelAI-style):
pythonSYSTEM_PROMPT = """You are an expert SQL generator for data analysis. You convert natural language questions into SQLite queries.

CONTEXT:
- Table name: {table_name}
- Schema: {schema_json}
- User question: {user_question}
- Conversation history: {conversation_history}

RULES:
1. Generate ONLY executable SQLite queries
2. Use proper SQLite syntax (no MySQL/PostgreSQL specific functions)
3. For aggregations, always include GROUP BY when needed
4. For trends over time, use appropriate date functions
5. Limit results to 1000 rows unless specifically asked for more
6. Use column names exactly as they appear in schema
7. For text comparisons, use LIKE with wildcards appropriately
8. Return NULL-safe queries using COALESCE when appropriate

QUERY TYPES:
- Simple select: SELECT column FROM table WHERE condition
- Aggregation: SELECT column, COUNT(*)/SUM()/AVG() FROM table GROUP BY column
- Time series: SELECT date_column, metric FROM table ORDER BY date_column
- Top N: SELECT column, COUNT(*) as count FROM table GROUP BY column ORDER BY count DESC LIMIT N
- Comparison: Use CASE statements for bucketing

OUTPUT FORMAT:
Return a JSON object:
{
  "sql": "SELECT ...",
  "query_type": "aggregation|filter|trend|comparison|summary",
  "explanation": "Brief explanation of what the query does",
  "requires_clarification": false,
  "clarification_question": null
}

If the question is ambiguous or lacks context, set requires_clarification to true and provide a clarification_question.
"""

USER_PROMPT_TEMPLATE = """
Table Schema:
{schema_details}

Recent conversation:
{conversation_history}

User question: "{user_question}"

Generate the appropriate SQL query following the rules above.
"""
Example Schema Format for Prompt:
pythonschema_details = """
Table: sales_data
Columns:
- date (DATE): Transaction date. Sample: 2024-01-15, 2024-01-16
- product (TEXT): Product name. Sample: Widget A, Widget B
- quantity (INTEGER): Quantity sold. Sample: 5, 10, 15
- revenue (REAL): Revenue in USD. Sample: 99.99, 149.50
- region (TEXT): Sales region. Sample: North, South, East, West

Total rows: 5000
Date range: 2024-01-01 to 2024-12-31
"""
Function Signature:
pythonasync def generate_sql(
    user_question: str,
    schema: dict,
    conversation_history: list,
    session_id: str
) -> dict:
    # Call Claude API with prompt
    # Parse response
    # Validate SQL syntax
    # Return structured result
```

---

### 4.4 Query Execution Module

**Responsibilities:**
- Execute SQL safely
- Handle errors gracefully
- Format results for visualization
- Apply row limits

**Logic Flow:**
```
1. Receive SQL query
2. Validate query (read-only, no DROP/DELETE/UPDATE)
3. Execute with timeout (5 seconds max)
4. Catch and handle errors
5. Format results as JSON
6. Return:
   {
     "success": true,
     "data": [...],
     "columns": [...],
     "row_count": 100,
     "execution_time_ms": 45
   }
Security Measures:

Read-only connection
Query timeout
No DDL statements allowed
Parameterized queries when needed


4.5 Visualization Engine
Responsibilities:

Analyze query results
Determine optimal chart type
Generate chart configuration
Return visualization spec

Chart Selection Logic:
pythondef determine_chart_type(data: list, columns: list, query_type: str) -> str:
    """
    Rules:
    - 1 numeric column → Bar chart (sorted)
    - 1 date + 1 numeric → Line chart
    - 1 categorical + 1 numeric → Bar chart
    - 2+ numerics → Multi-line chart or grouped bar
    - Proportions/percentages → Pie chart (if < 10 categories)
    - Comparisons → Horizontal bar chart
    - Distribution → Histogram
    - Trends over time → Line or area chart
    """
    
    num_cols = count_numeric_columns(columns)
    date_cols = count_date_columns(columns)
    cat_cols = count_categorical_columns(columns)
    
    if date_cols > 0 and num_cols >= 1:
        return "line"
    elif cat_cols == 1 and num_cols == 1:
        if len(data) <= 10:
            return "pie"
        return "bar"
    elif num_cols >= 2:
        return "multi_line"
    elif query_type == "trend":
        return "line"
    else:
        return "bar"
Visualization Prompt (for Claude):
pythonVIZ_PROMPT = """You are a data visualization expert. Analyze the query results and determine the best visualization.

QUERY RESULTS:
{results_json}

QUERY TYPE: {query_type}
ORIGINAL QUESTION: {user_question}

RULES:
1. Choose the most appropriate chart type: bar, line, pie, scatter, area, heatmap
2. Identify x-axis and y-axis columns
3. Suggest colors and formatting
4. Provide a descriptive title

OUTPUT FORMAT (JSON):
{
  "chart_type": "bar|line|pie|scatter|area",
  "config": {
    "x_axis": "column_name",
    "y_axis": "column_name" or ["col1", "col2"],
    "title": "Descriptive title",
    "x_label": "Label for X axis",
    "y_label": "Label for Y axis",
    "color_scheme": "blue|green|multi"
  },
  "insights": "Brief insight about the data pattern"
}
"""

4.6 Context Management Module
Responsibilities:

Store conversation history per session
Maintain uploaded file metadata
Track query history
Enable context-aware responses

Data Structures:
python# Session storage (in-memory or Redis)
session_data = {
    "session_id": "uuid",
    "file_name": "sales.csv",
    "table_name": "data",
    "schema": {...},
    "conversation": [
        {"role": "user", "content": "What are total sales?"},
        {"role": "assistant", "content": "...", "sql": "...", "viz": {...}}
    ],
    "created_at": "timestamp",
    "last_activity": "timestamp"
}
```

**Functions:**
- `create_session(file_metadata)` → session_id
- `get_session(session_id)` → session_data
- `add_to_history(session_id, user_msg, assistant_response)`
- `get_conversation_context(session_id, last_n=5)` → messages

---

## 5. API Endpoints

### 5.1 File Upload
```
POST /api/upload
Content-Type: multipart/form-data

Request:
- file: CSV or XLSX file

Response:
{
  "success": true,
  "session_id": "uuid",
  "file_name": "sales.csv",
  "schema": {...},
  "row_count": 5000,
  "preview": [first 10 rows]
}
```

### 5.2 Query Endpoint
```
POST /api/query
Content-Type: application/json

Request:
{
  "session_id": "uuid",
  "question": "What are the top 5 products by revenue?"
}

Response:
{
  "success": true,
  "answer": "The top 5 products by revenue are...",
  "sql": "SELECT product, SUM(revenue)...",
  "data": [...],
  "visualization": {
    "chart_type": "bar",
    "config": {...}
  },
  "requires_clarification": false
}
```

### 5.3 Session Management
```
GET /api/session/{session_id}
DELETE /api/session/{session_id}
GET /api/session/{session_id}/history
```

---

## 6. Detailed Prompt Engineering

### 6.1 Master System Prompt (CamelAI-inspired)
```
You are an AI Data Analyst assistant. Your role is to help users analyze their CSV/XLSX data through natural language conversation.

CAPABILITIES:
1. Understanding data analysis questions in plain English
2. Generating accurate SQL queries for SQLite
3. Explaining results in clear, business-friendly language
4. Suggesting appropriate visualizations
5. Asking clarifying questions when needed

WORKFLOW:
1. User uploads a file → You receive the schema
2. User asks a question → You:
   a. Understand the intent
   b. Generate SQL if needed
   c. Explain the results
   d. Suggest visualization
   e. Provide insights

RESPONSE STYLE:
- Be concise but complete
- Use business language, not technical jargon
- Provide context with numbers
- Suggest follow-up analyses when relevant
- Always validate your SQL against the schema

SCHEMA CONTEXT:
{schema_will_be_injected_here}

CONVERSATION HISTORY:
{history_will_be_injected_here}

Remember: You have access to only ONE table at a time. All queries must reference this table.
```

### 6.2 SQL Generation Prompt (Production-Ready)
```
TASK: Generate SQLite query from natural language.

SCHEMA:
Table: {table_name}
{column_details}

QUESTION: {user_question}

CONSTRAINTS:
- Use SQLite syntax only
- Column names are case-sensitive
- Date format: YYYY-MM-DD
- Use CAST for type conversions
- Limit to 1000 rows unless specified
- Use COALESCE for NULL handling

EXAMPLES:
Q: "What are total sales?"
SQL: SELECT SUM(revenue) as total_sales FROM {table_name}

Q: "Show me top 10 products"
SQL: SELECT product, SUM(revenue) as revenue FROM {table_name} GROUP BY product ORDER BY revenue DESC LIMIT 10

Q: "Sales trend by month in 2024"
SQL: SELECT strftime('%Y-%m', date) as month, SUM(revenue) as revenue FROM {table_name} WHERE date >= '2024-01-01' GROUP BY month ORDER BY month

YOUR TURN:
Generate SQL for: "{user_question}"

Return JSON:
{"sql": "...", "explanation": "...", "needs_clarification": false}
```

### 6.3 Clarification Prompt
```
The user asked: "{user_question}"

Available data: {schema_summary}

This question is ambiguous because: {reason}

Generate a clarification question that helps narrow down what the user wants.

Examples:
- "I see multiple date columns. Which date should I use: order_date or ship_date?"
- "Would you like to see this by day, week, or month?"
- "Should I include all regions or focus on a specific one?"

Your clarification question:
```

### 6.4 Insight Generation Prompt
```
QUERY RESULTS:
{results}

ORIGINAL QUESTION: {question}

Generate 2-3 key insights from these results. Focus on:
- Notable patterns or trends
- Unexpected findings
- Actionable takeaways

Format as bullet points, each 1-2 sentences.

7. Error Handling Strategy
7.1 Common Errors & Solutions
Error TypeHandling StrategyInvalid SQLRetry with corrected prompt, show generic error to userEmpty resultsInform user no data matches, suggest broader queryTimeoutSuggest filtering/limiting data, optimize querySchema mismatchRe-validate schema, ask user to re-uploadAmbiguous queryReturn clarification questionFile too largeReturn 413, suggest sampling
7.2 Error Response Format
json{
  "success": false,
  "error": {
    "code": "QUERY_ERROR",
    "message": "I couldn't analyze that data. Could you rephrase your question?",
    "technical_details": "SQL syntax error...",
    "suggestions": [
      "Try asking about specific columns",
      "Check if column names are spelled correctly"
    ]
  }
}
```

---

## 8. Performance Optimization

### 8.1 Caching Strategy
- Cache schema metadata per session
- Cache common query patterns
- Use in-memory storage for active sessions
- Clear sessions after 1 hour of inactivity

### 8.2 Query Optimization
- Auto-add LIMIT if missing (max 10,000 rows)
- Create indexes on frequently queried columns
- Use query explain plan for complex queries

### 8.3 LLM Call Optimization
- Batch visualization determination with SQL generation
- Use streaming responses for longer analyses
- Cache similar query patterns

---

## 9. Security Considerations

### 9.1 SQL Injection Prevention
- Whitelist-only SQL operations (SELECT)
- No dynamic table/column names from user input
- Parameterized queries when possible
- Validate all SQL before execution

### 9.2 File Upload Security
- File type validation (whitelist)
- Size limits (50MB max)
- Scan for malicious content
- Isolate session data per user

### 9.3 API Security
- Rate limiting: 60 requests/minute per session
- Session timeout: 1 hour
- API key validation for Claude
- CORS configuration

---

## 10. Testing Strategy

### 10.1 Test Cases

**Query Understanding:**
- Simple aggregations ("What's the total revenue?")
- Time-based queries ("Show sales in Q4 2024")
- Comparisons ("Compare North vs South region")
- Top N queries ("Top 10 customers")
- Ambiguous queries (should ask for clarification)

**SQL Generation:**
- Correct syntax for all query types
- Proper handling of NULL values
- Date formatting
- Aggregations with GROUP BY
- Multi-column queries

**Visualization:**
- Correct chart type selection
- Proper axis configuration
- Color scheme appropriateness

### 10.2 Test Data
Create sample CSV files:
- Sales data (1000 rows): date, product, quantity, revenue, region
- Customer data (500 rows): customer_id, name, signup_date, total_spent
- Time series (365 rows): daily metrics

---

## 11. Deployment Considerations

### 11.1 Environment Variables
```
ANTHROPIC_API_KEY=your_key
MAX_FILE_SIZE=52428800
SESSION_TIMEOUT=3600
ALLOWED_ORIGINS=http://localhost:3000
ENVIRONMENT=production
```

### 11.2 Scaling
- Use Redis for session storage (multi-instance support)
- Separate file storage service (S3/MinIO)
- Load balancer for API instances
- Queue system for long-running queries

### 11.3 Monitoring
- Track API response times
- Monitor Claude API usage/costs
- Log failed queries for analysis
- Alert on error rate spikes

---

## 12. Implementation Roadmap

### Phase 1: MVP (Week 1-2)
- File upload and parsing
- Basic SQL generation
- Simple query execution
- Text-only responses

### Phase 2: Visualization (Week 3)
- Chart type determination
- Visualization config generation
- Frontend integration

### Phase 3: Enhancement (Week 4)
- Conversation context
- Clarification questions
- Insight generation
- Error handling refinement

### Phase 4: Production (Week 5-6)
- Security hardening
- Performance optimization
- Comprehensive testing
- Deployment setup

---

## 13. Short Prompt for Cursor/Vibe Coding Tool
```
Build an AI Data Analyst backend in Python FastAPI that:

1. Accepts CSV/XLSX uploads, loads into SQLite
2. Uses Claude Sonnet 4.5 API to convert natural language questions to SQL
3. Executes queries and returns results with visualization configs (JSON)
4. Maintains session context and conversation history
5. Handles errors gracefully with clarification questions

Stack: FastAPI, pandas, sqlite3, anthropic SDK
Key features: File upload endpoint, query endpoint with NLP→SQL→Results→Viz pipeline, session management
Security: Read-only SQL, query validation, file size limits
Return visualization specs as JSON (chart type + config), not images

14. Critical Success Factors

Prompt Quality: The SQL generation prompt is 80% of success
Schema Context: Always pass complete schema to Claude
Error Recovery: Graceful handling when SQL fails
Visualization Logic: Smart chart type selection based on data shape
Context Maintenance: Track conversation for follow-up questions